{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Dataset Import\n",
    "\n",
    "In this notebook, we'll learn how to push a parallel text files to a huggingface dataset format. I have taken the example of Esperanto (epo) and English (eng) dataset. The similar methods could be applied to your dataset as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preparation\n",
    "\n",
    "The dataset used here is the eng-esp data from the Tatoeba challenge. All the available languages can be found [here](https://github.com/Helsinki-NLP/Tatoeba-Challenge/blob/master/Data.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-11-21 15:15:20--  https://object.pouta.csc.fi/Tatoeba-Challenge/eng-epo.tar\n",
      "Resolving object.pouta.csc.fi (object.pouta.csc.fi)... 86.50.254.18, 86.50.254.19\n",
      "Connecting to object.pouta.csc.fi (object.pouta.csc.fi)|86.50.254.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 35747840 (34M) [application/x-tar]\n",
      "Saving to: ‘eng-epo.tar’\n",
      "\n",
      "eng-epo.tar         100%[===================>]  34.09M  10.5MB/s    in 3.2s    \n",
      "\n",
      "2021-11-21 15:15:26 (10.5 MB/s) - ‘eng-epo.tar’ saved [35747840/35747840]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://object.pouta.csc.fi/Tatoeba-Challenge/eng-epo.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/eng-epo/\n",
      "data/eng-epo/train.src.gz\n",
      "data/eng-epo/dev.trg\n",
      "data/eng-epo/train.id.gz\n",
      "data/eng-epo/test.trg\n",
      "data/eng-epo/test.id\n",
      "data/eng-epo/dev.src\n",
      "data/eng-epo/dev.id\n",
      "data/eng-epo/test.src\n",
      "data/eng-epo/train.trg.gz\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf  'eng-epo.tar'\n",
    "!gunzip 'data/eng-epo/train.src.gz'\n",
    "!gunzip 'data/eng-epo/train.trg.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv 'data/eng-epo/train.src' 'data/eng-epo/train.eng'\n",
    "!mv 'data/eng-epo/train.trg' 'data/eng-epo/train.epo'\n",
    "!mv 'data/eng-epo/dev.src' 'data/eng-epo/dev.eng'\n",
    "!mv 'data/eng-epo/test.src' 'data/eng-epo/test.eng'\n",
    "!mv 'data/eng-epo/dev.trg' 'data/eng-epo/dev.epo'\n",
    "!mv 'data/eng-epo/test.trg' 'data/eng-epo/test.epo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "We will create a new text file in which each line will contain two lines from the corpora which are tab separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parallel_text_files(src_file, trg_file, new_file):\n",
    "\n",
    "    with open(src_file, \"r\") as src, open(trg_file, \"r\") as trg, open(\n",
    "        new_file, \"w\"\n",
    "    ) as new_f:\n",
    "        for src_line, trg_line in zip(src, trg):\n",
    "            new_f.write(src_line + \"\\t\" + trg_line + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parallel_text_files(\n",
    "    \"data/eng-epo/train.eng\", \"data/eng-epo/train.epo\", \"data/eng-epo/train.eng-epo.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parallel_text_files(\n",
    "    \"data/eng-epo/dev.eng\", \"data/eng-epo/dev.epo\", \"data/eng-epo/dev.eng-epo.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_parallel_text_files(\n",
    "    \"data/eng-epo/test.eng\", \"data/eng-epo/test.epo\", \"data/eng-epo/test.eng-epo.txt\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the text files into the HuggingFace Dataset\n",
    "After creating the above files, these files could easily be loaded into the HuggingFace dataset module and then be used further for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"text\",\n",
    "    data_files={\n",
    "        \"train\": \"data/eng-epo/train.eng-epo.txt\",\n",
    "        \"dev\": \"data/eng-epo/dev.eng-epo.txt\",\n",
    "        \"test\": \"data/eng-epo/test.eng-epo.txt\",\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ee6759b9d6e8105c157b4d850d6f7f326467de02dec6fbe61f5d7e4feab800cd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('google_asr': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
